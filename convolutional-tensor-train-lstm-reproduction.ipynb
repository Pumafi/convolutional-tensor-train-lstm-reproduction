{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, AvgPool2D, Layer, Add\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"./data/mnist_test_seq.npy\"\n",
    "\n",
    "if not os.path.exists('./data'):\n",
    "    os.mkdir('./data')\n",
    "    \n",
    "# Download data\n",
    "if not os.path.exists(dataset):\n",
    "    r = requests.get(\"http://www.cs.toronto.edu/~nitish/unsupervised_video/mnist_test_seq.npy\")\n",
    "    open(dataset, 'wb').write(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "def plot_sequence_images(image_array):\n",
    "    dpi = 72.0\n",
    "    xpixels, ypixels = image_array[0].shape[:2]\n",
    "    fig = plt.figure(figsize=(ypixels/dpi, xpixels/dpi), dpi=dpi)\n",
    "    im = plt.figimage(image_array[0])\n",
    "\n",
    "    def animate(i):\n",
    "        if image_array[i].shape[-1] == 1:\n",
    "            image = image_array[i][:,:,0]\n",
    "        else:\n",
    "            image = image_array[i]\n",
    "        im.set_array(image)\n",
    "        return (im,)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(image_array), interval=33, repeat_delay=1, repeat=True)\n",
    "    display(HTML(anim.to_html5_video()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "\n",
    "assert os.path.exists(dataset), \"ASSERT ERROR: The file does not exist.\"\n",
    "data = np.float32(np.load(dataset) / 255.0)\n",
    "data_samples = data.shape[0]\n",
    "\n",
    "data_frames  = data.shape[1]\n",
    "data = np.swapaxes(data, 0, 1)\n",
    "data = np.expand_dims(data, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"64\" height=\"64\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAAL0m1kYXQAAAKuBgX//6rcRem9\n",
       "5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTU1IHIyOTE3IDBhODRkOTggLSBILjI2NC9NUEVHLTQg\n",
       "QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE4IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n",
       "eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n",
       "MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n",
       "PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n",
       "b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9MiBsb29r\n",
       "YWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFj\n",
       "ZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJh\n",
       "bWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdl\n",
       "aWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MjUgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVz\n",
       "aD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBx\n",
       "cG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAFyZYiE\n",
       "AH/+wn/mWTOXlV/zef7P+QABw0kPxyuAGejYRJHuXl11aNdHNzT2OKlGNPA2LPpusVw3FWiaQ9q7\n",
       "y8oKAHUCcdpQPGGbFYn/yvuGDwzWn0i2bw4sjlZIJmsXlrm5g4Smfd9qzF6zN0h1ZiDJy/GqKCvf\n",
       "XkEhTL4lP/1d9D8BXeko7lH+NC2lYlCOij29MyQ1ZXFk/vRLNyGfeU6X00t8+tw8lRRkkAgosTMn\n",
       "cH5FhKTuAp8zq6dVR68cOrwKG0++x/HvaoNVbzK6zCvsMB0ZB1MZrzD/CdeNh8kH7t2ufgYyHoXm\n",
       "2lLu66xkE7WYcRClztFKmjqRPqDuFQx4GF1gCf5N7gEccBbkDfAxTGRWvp8eBUehQVs2k4j/tMqs\n",
       "QZlt28UUcPW5AJiaHO+GLxaGMY1ZyFySworE8eocz6YKOg4z16I8jELK2kcrKy1QbNmnNSgwalSH\n",
       "RjTxNM1wJNFwfoPGyI8s4UqmBSMm/GTHOQAAAY1BmiRsQz+MhqyuxbDc50d8n9/LFLxaAESG4dWH\n",
       "XTpYDU9uCdAEf3NJEppbLNI1K7JMHJtzZNAcZ9/kTud0+Bo0bYXlP3qiTybKfzrakkxxVCDXfhQd\n",
       "6yfitkyFDziqQH1iUsTIVi4m7CT+1HdcqzMCy9z2MfFmtlLDjS670yLPDDCAsfpaJhx++5uyNzaf\n",
       "wld2d8B1qdW5St/stva0wl1o5iv3LwBof1+JhQUWh+2ksmGWJCLeBhp/GB3E2cFvQpoeIQdSrBRJ\n",
       "EgvSD8lTykZ/EIMIMZgrVn0yL7mmC0FMHZQo7Gg0iKNb8LriL6pIxlXnenCbyioeLWkjeTwCNZRU\n",
       "mZ2vqeAU/1LvMhi64fhyixZKQ2Yu/98xqlHMztnkpeCmYHMymauCxkjlbmpJeMkyZbmHvrQzHAFc\n",
       "700V50opeA6m1+VINoUHtMnvG3H4tW7d2ayWWzVTpnhLB8XI4TAV8lVSQTe+S81fwW1Gy2SMU82E\n",
       "Kvw6EmCH713TJYpwonvByHFi38JBjL04j8u4AAAAVkGeQniE//dfB7wAj0wAaS35BFxySDzb9vUI\n",
       "8W+pzy+PEReqHlvC6M4j/qyKRVdioRlCRmoiFcC83Flzc6nxmuRSxZWJKsTCVZp6sGYRS89IQ9AN\n",
       "GP+5AAAATQGeYXRF/8iUG65J1AGdYhBEaARd1NojumqtE+e73Y07kz7AyHSlBbIyOA50Vk40Ri+5\n",
       "nNkNaXlxH/qriHYESWv7CQyQhLbbrbCSdyqcAAAALAGeY2pF//qw/owGRJZSiZQQ+Ar7oZPMACKE\n",
       "vGT0Wvd0h4iZmAHJstEugeT7AAAAsEGaZUmoQWiZTAh3/5BxhA22WEDABnzRFhCRsA++0qa2ihDt\n",
       "C0DN1mGUYPE3JVLi7GQ4mduHMKbT7ms1MigP2+rmCWB1B3n8V9YvAKcIDKdauiGKw90A4iE/VPRo\n",
       "5rPnD69ME+aysX+07oBw9jjmC9z647fzmms2wKFrU8mYEyUfyWsoC0Tix1idZoFyQvsNiOseZ8+Y\n",
       "JRt57SINv4v5fvNCvnBa0ZmbZ0Y/k63wPe+BAAAAZ0GaiEnhClJlMCFfiTzqfq4Sd9NQSDOXE08g\n",
       "h0x3h7XGPCq/M6WvILKsp5nA/vLDXBiNE2ii67cS1wOt1UXpmAOCivo94gRArh2p/hvQ/8cKtY5K\n",
       "1w1qFewYbo6QcGlXFsJOv/aL3uEAAAA6QZ6mRTRMI/+phxjg0MfR2af/SFchYASXzdVZXkRlKUba\n",
       "i7YS4kjN9I0FgOjtCl4h1JABhdQR9Y5lzQAAACsBnsdqRf950Vi5TEWiuAGj0LL01F50zUcr+ubN\n",
       "kjuJR2fJLlz+L2s4On1MAAAARkGayUmoQWiZTAhf//UPpmnVgBWJvinI+xh7nvCaUiaYVkLCRB/G\n",
       "Ck0NGeG8aEFSxBnNxof38t0ntLjxycPMnZKMYXyhecAAAABYQZrrSeEKUmUwURLG/8EtjOB9PHy4\n",
       "ldvQuXm9zs/eYlnuY3H1SpW7Qq0/ryuejEc/2oU/CgOrlJvpMCYpfm5AP+eGZ7rUGgZd1W2CdmDH\n",
       "/NMhARm3/Mu7bwAAAEABnwpqRf+4O/dR0HtAE7i1WQcwS0nbAoXfCdp68DYzkCKax4rpDHBhQr2S\n",
       "9ZFKYj3UvR2IvfPJoH7vUfMdwV1gAAAAQ0GbDEnhDomUwIv/1DYnC+wCfHbHssZQMQ/5XwyWuXQA\n",
       "zQzdhg2gGIyEo+qUQjThFEtLh91fjABMnvntA/0CqbDeYv4AAACOQZstSeEPJlMCL//REjG8gvYA\n",
       "j0qO7cYQaRiMssyRBKzcr8ThxQ2LpZy0NnuL+B5UoRkKWMLRZFnQqwDNGJv5Zm1s5mtwRNBzbU91\n",
       "Ep9Z8Seq79lWTgHnivYcdPiWzNtdD1U6J8sjknl0XcURvSqgzlRvE85lpgoyg/222ff8VTplnD+o\n",
       "/DZNfttW0Gm6fQAAAIRBm05J4Q8mUwIv/8inDb+5idgQmmWcAVTTjnhkb4lhgVs8ZtsorffUGrmz\n",
       "tyHAin9OfFD0Dbp4bnEIQQjuXRUIYBGuIDTmbLcTFHFkAxn4q/3CrASL6yzGiP9fCYJYhav4fQll\n",
       "InmlNWJbjvsTGuznipsOlNxGv6gg1Tmr74a25VJVP5EAAABZQZtvSeEPJlMCL//XOsdvLPYARquP\n",
       "VyREcgbDdOGeplryalfXLF2uxh8zSKmShqeV06sQEsdbSDExfOwxF9d84wyCKtIQJYTynncIWe9Z\n",
       "VIRBFRgU8SxKDUEAAABwQZuQSeEPJlMCH//qIK+1U9usfK7/uaFBs70frtT+A9ugB0rqMcsOXnr2\n",
       "36zl0yvS6NgDPpJMQPzpDqQJsf/njMBJmVHW0QJ6mHI7pL+ESzzWDGLZn1CSz3PvU3Rx8oU4QKpQ\n",
       "BvX/EeRztKX6rz0/wAAAAClBm7NJ4Q8mUwIn/88/2Xbfn1VylAQ/hq+tYj1+rYAny9hFNTPlK+Lr\n",
       "VAAAADJBn9FFETwh//ciTQ5Uq3C+9P2oT3dgBAKsDArVxtQfon+uIbtOdqdfsthe8SJlg5o+VQAA\n",
       "ACcBn/JqS//zO6DqeQs9SM8fn3FAHN77KJfYb8fr40G1v1l/IUX06/QAAAPsbW9vdgAAAGxtdmhk\n",
       "AAAAAAAAAAAAAAAAAAAD6AAAApQAAQAAAQAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAA\n",
       "AAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAAAxZ0cmFrAAAAXHRr\n",
       "aGQAAAADAAAAAAAAAAAAAAABAAAAAAAAApQAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAA\n",
       "AAABAAAAAAAAAAAAAAAAAABAAAAAAEAAAABAAAAAAAAkZWR0cwAAABxlbHN0AAAAAAAAAAEAAAKU\n",
       "AAAEIAABAAAAAAKObWRpYQAAACBtZGhkAAAAAAAAAAAAAAAAAAA+gAAAKUBVxAAAAAAALWhkbHIA\n",
       "AAAAAAAAAHZpZGUAAAAAAAAAAAAAAABWaWRlb0hhbmRsZXIAAAACOW1pbmYAAAAUdm1oZAAAAAEA\n",
       "AAAAAAAAAAAAACRkaW5mAAAAHGRyZWYAAAAAAAAAAQAAAAx1cmwgAAAAAQAAAflzdGJsAAAAsXN0\n",
       "c2QAAAAAAAAAAQAAAKFhdmMxAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAEAAQABIAAAASAAAAAAA\n",
       "AAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGP//AAAAL2F2Y0MBZAAK/+EAFmdk\n",
       "AAqs2UQmhAAAAwCEAAAfQDxIllgBAAZo6+PLIsAAAAAcdXVpZGtoQPJfJE/FujmlG88DI/MAAAAA\n",
       "AAAAGHN0dHMAAAAAAAAAAQAAABQAAAIQAAAAFHN0c3MAAAAAAAAAAQAAAAEAAACAY3R0cwAAAAAA\n",
       "AAAOAAAAAQAABCAAAAABAAAKUAAAAAEAAAQgAAAAAQAAAAAAAAABAAACEAAAAAEAAAQgAAAAAQAA\n",
       "CEAAAAACAAACEAAAAAEAAAQgAAAAAQAABjAAAAABAAACEAAAAAUAAAQgAAAAAQAACEAAAAACAAAC\n",
       "EAAAABxzdHNjAAAAAAAAAAEAAAABAAAAFAAAAAEAAABkc3RzegAAAAAAAAAAAAAAFAAABCgAAAGR\n",
       "AAAAWgAAAFEAAAAwAAAAtAAAAGsAAAA+AAAALwAAAEoAAABcAAAARAAAAEcAAACSAAAAiAAAAF0A\n",
       "AAB0AAAALQAAADYAAAArAAAAFHN0Y28AAAAAAAAAAQAAACwAAABidWR0YQAAAFptZXRhAAAAAAAA\n",
       "ACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAdZGF0YQAA\n",
       "AAEAAAAATGF2ZjU4LjI5LjEwMA==\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 64x64 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sequence_images(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 20, 64, 64, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_samples, data_frames)\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTTLSTMLayer(Layer):\n",
    "    def __init__(self, input_channels, hidden_channels,\n",
    "                      order=3, steps=3, ranks=8,\n",
    "                      kernel_size=(5, 5), bias=True):\n",
    "        super(ConvTTLSTMLayer, self).__init__()\n",
    "        \n",
    "        ## Input/output interfaces\n",
    "        self.input_channels  = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        \n",
    "        # Number of hidden states taken\n",
    "        self.steps = steps\n",
    "        # Number of G matrices\n",
    "        self.order = order\n",
    "        # How many hiddent step by group to be preprocessed together\n",
    "        self.lags = steps - order + 1\n",
    "        \n",
    "        ## Convolutional operations\n",
    "        padding     = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        \n",
    "        # Change to 3D ? Same effecct but different optimisations\n",
    "        preprocessing_Conv2D = lambda out_channels:  Conv2D(out_channels,\n",
    "                                                            kernel_size=kernel_size,\n",
    "                                                            padding=padding + (0,),\n",
    "                                                            use_bias=bias)\n",
    "    \n",
    "        tt_Conv2D = lambda out_channels:  Conv2D(out_channels,\n",
    "                                                  kernel_size=kernel_size,\n",
    "                                                  padding=padding,\n",
    "                                                  use_bias=bias)\n",
    "\n",
    "        self.layers_preprocess = []\n",
    "        self.layers_conv2d = []\n",
    "        \n",
    "        for l in range(order):\n",
    "            self.layers_preprocess.append(preprocessing_Conv2D( \n",
    "                out_channels = ranks if l < order - 1 else 4 * hidden_channels))\n",
    "            \n",
    "            self.layers_conv2d.append(tt_Conv2D(out_channels = ranks))\n",
    "        \n",
    "        def initialize(self, inputs):\n",
    "            '''\n",
    "            Initialize the hidden cell states H\n",
    "            '''\n",
    "            \n",
    "            batch_size, input_channels, height, width = inputs.size()\n",
    "            \n",
    "            self.hidden_states = [tf.zeros([batch_size, \n",
    "                height, width, self.hidden_channels])] * self.steps\n",
    "            self.hidden_state_index = 0\n",
    "            self.cell_states = tf.zeros([batch_size,\n",
    "                height, width, self.hidden_channels])\n",
    "        \n",
    "        def call(self, inputs, first_step = False):\n",
    "            '''\n",
    "            Call the model\n",
    "            '''\n",
    "            if first_step:\n",
    "                self.initialize(inputs)\n",
    "                first_step = False\n",
    "            \n",
    "            ## Preprocessing + Convolutional tensor-train module\n",
    "            ## Algorithm 2\n",
    "            for i in range(self.order):\n",
    "                input_pointer = self.hidden_state_index if not i else (input_pointer + 1) % self.steps\n",
    "                \n",
    "                # Start hidden states input at pointer but wrap around\n",
    "                input_states = self.hidden_states[input_pointer:] + self.hidden_states[:input_pointer]\n",
    "                \n",
    "                # Take one group of Hidden States\n",
    "                input_states = input_states[:self.lags]\n",
    "                \n",
    "                input_states = tf.stack(input_states, dim=-1)\n",
    "                input_states = self.layers_preprocess[l](input_states)\n",
    "                \n",
    "                if i == 0:\n",
    "                    temp_states = input_states\n",
    "                else: # if i > 0\n",
    "                    temp_states = input_states + self.layers_conv2d[l - 1](temp_states)\n",
    "                \n",
    "            ## Standard convolutional-LSTM module\n",
    "            concat_conv = self.layers_conv2d(tf.concat([inputs, temps_states], dim=-1))\n",
    "            cc_i, cc_f, cc_o, cc_g = tf.split(concat_conv, self.hidden_channels, dim = -1)\n",
    "            \n",
    "            \n",
    "            i = tf.math.sigmoid(cc_i)\n",
    "            f = tf.math.sigmoid(cc_f)\n",
    "            o = tf.math.sigmoid(cc_o)\n",
    "            g = tf.math.tanh(cc_g)\n",
    "            \n",
    "            self.cell_states = f * self.cell_states + i * g\n",
    "            outputs = o * torch.tanh(self.cell_states)\n",
    "            self.hidden_states[self.hidden_state_index] = outputs\n",
    "            self.hidden_pointer = (self.hidden_state_index + 1) % self.steps\n",
    "\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "convttlstm = ConvTTLSTMLayer(5,3)\n",
    "convttlstm.build((None, *input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTTLSTMNet(Model):\n",
    "    def __init__(self,\n",
    "                 input_channels,\n",
    "                 layers_per_block:list, hidden_channels:list, skip_stride = None,\n",
    "                 # Params for the TTLSTM\n",
    "                 order=3, steps=3, ranks=8,\n",
    "                 kernel_size = (3,3), bias = True,\n",
    "                 output_sigmoid = False):\n",
    "        super(ConvTTLSTMNet, self).__init__()\n",
    "        \n",
    "        ## Hyperparameters\n",
    "        self.layers_per_block = layers_per_block\n",
    "        self.hidden_channels  = hidden_channels\n",
    "\n",
    "        self.num_blocks = len(layers_per_block)\n",
    "        assert self.num_blocks == len(hidden_channels), \"Invalid number of blocks.\"\n",
    "        \n",
    "        \n",
    "        self.skip_stride = (self.num_blocks + 1) if skip_stride is None else skip_stride\n",
    "\n",
    "        self.output_sigmoid = output_sigmoid\n",
    "        \n",
    "        self.layers_ = {}\n",
    "        \n",
    "        # Model Architecture\n",
    "        for b in range(self.num_blocks):\n",
    "            for l in range(layers_per_block[b]):\n",
    "                # number of input channels to the current layer\n",
    "                if l > 0:\n",
    "                    channels = hidden_channels[b]\n",
    "                elif b == 0:\n",
    "                    channels = input_channels\n",
    "                else:\n",
    "                    # First layer of a block after the first one\n",
    "                    channels = hidden_channels[b - 1]\n",
    "                \n",
    "                # We keep the layer id for skip connections\n",
    "                # FIXME ? Maybe there's something to do that with Keras ? Unsure\n",
    "                lid = \"b{}l{}\".format(b, l)\n",
    "                \n",
    "                cttlstmlayer = ConvTTLSTMLayer(\n",
    "                    input_channels = channels, hidden_channels = hidden_channels[b],\n",
    "                    order = order, steps = steps, ranks = ranks, \n",
    "                    kernel_size = kernel_size, bias = bias)\n",
    "                \n",
    "                self.layers_[lid] = cttlstmlayer\n",
    "                \n",
    "        # Last Layer\n",
    "        # nb of input\n",
    "        channels = hidden_channels[-1]\n",
    "        \n",
    "        # ???????\n",
    "        # Si il y a plus de blocks que la distance de skip, alots on ajoute Ã  l'input\n",
    "        # le skip ? FIXME ==> Essayez de comprendre\n",
    "        if self.num_blocks >= self.skip_stride:\n",
    "            channels += hidden_channels[-1-self.skip_stride]\n",
    "            \n",
    "        if self.output_sigmoid:\n",
    "            activation=\"sigmoid\"\n",
    "        else:\n",
    "            activation=None\n",
    "            \n",
    "        self.layers_[\"output\"] = Conv2D(input_channels, kernel_size=(1,1), \n",
    "                                       activation=activation, padding=\"same\", use_bias=True)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, input_frames:int, future_frames:int, output_frames:int,\n",
    "                teacher_forcing = False, scheduled_sampling_ratio = 0):\n",
    "            # tf is teacher forcing ?\n",
    "        if teacher_forcing and scheduled_sampling_ratio > 1e-6:\n",
    "            teacher_forcing_mask = np.random.binomial(size=(inputs.size(0), future_frames - 1, 1, 1, 1), n=1, p=scheduled_sampling_ratio)\n",
    "        else:\n",
    "            teacher_forcing = False\n",
    "            \n",
    "        total_steps = input_frames + future_frames - 1\n",
    "        outputs = [None] * total_steps\n",
    "\n",
    "        for t in range(total_steps):\n",
    "            if t < input_frames: \n",
    "                input_ = inputs[:, t]\n",
    "            elif not teacher_forcing:\n",
    "                input_ = outputs[t-1]\n",
    "            else: # if t >= input_frames and teacher_forcing:\n",
    "                mask = teacher_forcing_mask[:, t - input_frames]\n",
    "                input_ = inputs[:, t] * mask + outputs[t-1] * (1 - mask)\n",
    "\n",
    "            print(input_.shape)\n",
    "            first_step = (t == 0)\n",
    "\n",
    "            queue = []\n",
    "\n",
    "            for b in range(self.num_blocks):\n",
    "                for l in range(self.layers_per_block[b]):\n",
    "                    lid = \"b{}l{}\".format(b, l)\n",
    "                    input_ = self.layers_[lid](input_, first_step = first_step)\n",
    "\n",
    "                queue.append(input_)\n",
    "                if b >= self.skip_stride:\n",
    "                    input_ = tf.concat([input_, queue.pop(0)], dim=1)\n",
    "\n",
    "\n",
    "            \n",
    "            outputs[t] = self.layers_[\"output\"](input_)\n",
    "\n",
    "        outputs = outputs[-output_frames:]\n",
    "\n",
    "        outputs = tf.stack([outputs[t] for t in range(output_frames)], axis = 1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n",
      "(50, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "image_channels = 1\n",
    "batch_size = 50\n",
    "predict_frames = 50\n",
    "input_shape = (batch_size, 20, 1, 64, 64)\n",
    "batch = data[:batch_size]\n",
    "convttlstm = ConvTTLSTMNet(1, [3,3,3,3], [32,48,48,32])\n",
    "outputs = convttlstm.call(batch, 20, predict_frames, predict_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([50, 50, 64, 64, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"64\" height=\"64\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAAAHmm1kYXQAAAKuBgX//6rcRem9\n",
       "5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTU1IHIyOTE3IDBhODRkOTggLSBILjI2NC9NUEVHLTQg\n",
       "QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE4IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n",
       "eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n",
       "MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n",
       "PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n",
       "b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9MiBsb29r\n",
       "YWhlYWRfdGhyZWFkcz0xIHNsaWNlZF90aHJlYWRzPTAgbnI9MCBkZWNpbWF0ZT0xIGludGVybGFj\n",
       "ZWQ9MCBibHVyYXlfY29tcGF0PTAgY29uc3RyYWluZWRfaW50cmE9MCBiZnJhbWVzPTMgYl9weXJh\n",
       "bWlkPTIgYl9hZGFwdD0xIGJfYmlhcz0wIGRpcmVjdD0xIHdlaWdodGI9MSBvcGVuX2dvcD0wIHdl\n",
       "aWdodHA9MiBrZXlpbnQ9MjUwIGtleWludF9taW49MjUgc2NlbmVjdXQ9NDAgaW50cmFfcmVmcmVz\n",
       "aD0wIHJjX2xvb2thaGVhZD00MCByYz1jcmYgbWJ0cmVlPTEgY3JmPTIzLjAgcWNvbXA9MC42MCBx\n",
       "cG1pbj0wIHFwbWF4PTY5IHFwc3RlcD00IGlwX3JhdGlvPTEuNDAgYXE9MToxLjAwAIAAAAFkZYiE\n",
       "AH/+wn/mWSpqNXhNt6ApkOw2H9owYoqwBRDlDLYODmG+zDJ7Q5bzJmESOe9+HnHNsKLCrqXp8cL2\n",
       "gcYPR0GQlefrFzOshU6CugBsWFvLAvkPH4wf1ATerwJoUGHvM7dipJ+YM//rNO3HjMCJjPfYKK2K\n",
       "/wNn+/JcKvK9qR50wtSKUt+tsRwCA32tM1RxveASzeSX6H2DnIW1GVq4pNGisuve+GT50wIyFh2y\n",
       "26JhamaXEHkiVfbujCvIxXGH/Ox0/41HZkDQpLPV5kR48TbsWC73WGYkPcewymg9G33B4w6718xV\n",
       "41AdWZM74mH2R7kCLKubupdL7zH2qKoG0HV4es10/53fAhLwMTAQSo1ZKJulx9jb/WbJS/iVN/Vp\n",
       "wcbAsiKsT739xRuBPl4ErIeoVEvq/p96TXgfSrFdyozrn5/0gSfAjylsX6ToN4Sjr00zzaPB3eDt\n",
       "p+vCok7A+hz9WUkAAABrQZoiYh5gH5BMDYBgAJlH//lU4fyoAHB/8b7z05xouzh98YjAjfvXg/26\n",
       "7oGxTOdm6ef+B+/IdxkvBSIiwjpifsZ7DliUjswQbyMbgP5X6ycfQUGHQskcyJ32n4reFBQHRM+i\n",
       "iy3wYLFY43gAAAAXAZ5BeRv/zOoSP14HcQKNhMCnfpMd1+EAAAA2QZpESeEIQyCBEC8A/pAJgKgH\n",
       "5AvAPwCL//kzEGAQQB610NyNVvXsEBB5LHlquBi4f0a86NXAAAAAFgGeY2pG/2jr+bqVgkbjf2h7\n",
       "huFYbzkAAAAmQYiZQN/+4QP4FNYocGN8xHREPFcuRtxKO/UR5DhOpO/weOxyIKsAAAAPQZqJSeEP\n",
       "JlMCF//+jNNhAAAACUGep0URPH8VcQAAAAgBnsZ0Rv8ZEAAAAAgBnshqRv8ZEAAAABBBms1JqEFo\n",
       "mUwIX//+jNNhAAAACUGe60URLH8VcAAAAAgBnwp0Rv8ZEAAAAAgBnwxqRv8ZEQAAABBBmxFJqEFs\n",
       "mUwIX//+jNNhAAAACUGfL0UVLH8VcQAAAAgBn050Rv8ZEAAAAAgBn1BqRv8ZEAAAABBBm1VJqEFs\n",
       "mUwIV//+OI3BAAAACUGfc0UVLH8VcAAAAAgBn5J0Rv8ZEAAAAAgBn5RqRv8ZEQAAABBBm5lJqEFs\n",
       "mUwIV//+OI3AAAAACUGft0UVLH8VcQAAAAgBn9Z0Rv8ZEQAAAAgBn9hqRv8ZEAAAABBBm91JqEFs\n",
       "mUwIV//+OI3BAAAACUGf+0UVLH8VcAAAAAgBnhp0Rv8ZEQAAAAgBnhxqRv8ZEQAAABBBmgFJqEFs\n",
       "mUwIV//+OI3AAAAACUGeP0UVLH8VcAAAAAgBnl50Rv8ZEQAAAAgBnkBqRv8ZEAAAABBBmkVJqEFs\n",
       "mUwIV//+OI3BAAAACUGeY0UVLH8VcAAAAAgBnoJ0Rv8ZEQAAAAgBnoRqRv8ZEQAAABBBmolJqEFs\n",
       "mUwIV//+OI3BAAAACUGep0UVLH8VcQAAAAgBnsZ0Rv8ZEAAAAAgBnshqRv8ZEAAAABBBms1JqEFs\n",
       "mUwIT//98a2BAAAACUGe60UVLH8VcAAAAAgBnwp0Rv8ZEAAAAAgBnwxqRv8ZEQAAAA5BmxFJqEFs\n",
       "mUwI3/pbtwAAAAlBny9FFSx/FXEAAAAIAZ9OdEb/GRAAAAAIAZ9Qakb/GRAAAAWEbW9vdgAAAGxt\n",
       "dmhkAAAAAAAAAAAAAAAAAAAD6AAABnIAAQAAAQAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAB\n",
       "AAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAABK50cmFrAAAA\n",
       "XHRraGQAAAADAAAAAAAAAAAAAAABAAAAAAAABnIAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAA\n",
       "AAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAEAAAABAAAAAAAAkZWR0cwAAABxlbHN0AAAAAAAAAAEA\n",
       "AAZyAAAEIAABAAAAAAQmbWRpYQAAACBtZGhkAAAAAAAAAAAAAAAAAAA+gAAAZyBVxAAAAAAALWhk\n",
       "bHIAAAAAAAAAAHZpZGUAAAAAAAAAAAAAAABWaWRlb0hhbmRsZXIAAAAD0W1pbmYAAAAUdm1oZAAA\n",
       "AAEAAAAAAAAAAAAAACRkaW5mAAAAHGRyZWYAAAAAAAAAAQAAAAx1cmwgAAAAAQAAA5FzdGJsAAAA\n",
       "sXN0c2QAAAAAAAAAAQAAAKFhdmMxAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAEAAQABIAAAASAAA\n",
       "AAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGP//AAAAL2F2Y0MBZAAK/+EA\n",
       "FmdkAAqs2UQmhAAAAwCEAAAfQDxIllgBAAZo6+PLIsAAAAAcdXVpZGtoQPJfJE/FujmlG88DI/MA\n",
       "AAAAAAAAGHN0dHMAAAAAAAAAAQAAADIAAAIQAAAAFHN0c3MAAAAAAAAAAQAAAAEAAAGgY3R0cwAA\n",
       "AAAAAAAyAAAAAQAABCAAAAABAAAGMAAAAAEAAAIQAAAAAQAABjAAAAABAAACEAAAAAEAAAQgAAAA\n",
       "AQAAClAAAAABAAAEIAAAAAEAAAAAAAAAAQAAAhAAAAABAAAKUAAAAAEAAAQgAAAAAQAAAAAAAAAB\n",
       "AAACEAAAAAEAAApQAAAAAQAABCAAAAABAAAAAAAAAAEAAAIQAAAAAQAAClAAAAABAAAEIAAAAAEA\n",
       "AAAAAAAAAQAAAhAAAAABAAAKUAAAAAEAAAQgAAAAAQAAAAAAAAABAAACEAAAAAEAAApQAAAAAQAA\n",
       "BCAAAAABAAAAAAAAAAEAAAIQAAAAAQAAClAAAAABAAAEIAAAAAEAAAAAAAAAAQAAAhAAAAABAAAK\n",
       "UAAAAAEAAAQgAAAAAQAAAAAAAAABAAACEAAAAAEAAApQAAAAAQAABCAAAAABAAAAAAAAAAEAAAIQ\n",
       "AAAAAQAAClAAAAABAAAEIAAAAAEAAAAAAAAAAQAAAhAAAAABAAAKUAAAAAEAAAQgAAAAAQAAAAAA\n",
       "AAABAAACEAAAABxzdHNjAAAAAAAAAAEAAAABAAAAMgAAAAEAAADcc3RzegAAAAAAAAAAAAAAMgAA\n",
       "BBoAAABvAAAAGwAAADoAAAAaAAAAKgAAABMAAAANAAAADAAAAAwAAAAUAAAADQAAAAwAAAAMAAAA\n",
       "FAAAAA0AAAAMAAAADAAAABQAAAANAAAADAAAAAwAAAAUAAAADQAAAAwAAAAMAAAAFAAAAA0AAAAM\n",
       "AAAADAAAABQAAAANAAAADAAAAAwAAAAUAAAADQAAAAwAAAAMAAAAFAAAAA0AAAAMAAAADAAAABQA\n",
       "AAANAAAADAAAAAwAAAASAAAADQAAAAwAAAAMAAAAFHN0Y28AAAAAAAAAAQAAACwAAABidWR0YQAA\n",
       "AFptZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0\n",
       "b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU4LjI5LjEwMA==\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 64x64 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sequence_images(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
